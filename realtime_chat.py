// Whisper,ChatGPT, TTS ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì‹¤ì‹œê°„ ëŒ€í™” ì‹œìŠ¤í…œ ver.2.3
// ë§ˆì´í¬ ìŒì„± ì…ë ¥ -> Whisperë¡œ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ -> GPT ì‘ë‹µ ìƒì„± -> ì‘ë‹µ(í…ìŠ¤íŠ¸)ì„ TTSë¡œ ë³€í™˜ -> ìŒì„± ì¶œë ¥
// ì‚¬ìš©ì ë°œí™” : user_input.wav ìƒì„±
// TTS ìŒì„± : response.wav ìƒì„±
// ìŒì„± ê°ì§€ ì‹œ ìŒì„± ë…¹ìŒ ì‹œì‘
// silence_durationì„ ë„˜ì–´ê°€ë©´ ìŒì„± ê°ì§€ ì¢…ë£Œ
// ì¢…ë£Œ(Ctrl + C) ì…ë ¥ ì „ê¹Œì§€ ëŒ€í™” ê¸°ëŠ¥ ë°˜ë³µ

import whisper
from torch.serialization import add_safe_globals
from TTS.tts.configs.xtts_config import XttsConfig, XttsAudioConfig
from TTS.config.shared_configs import BaseDatasetConfig
from TTS.tts.models.xtts import XttsArgs
add_safe_globals({XttsConfig, XttsAudioConfig, BaseDatasetConfig, XttsArgs})
from TTS.api import TTS
import os
import sounddevice as sd
import soundfile as sf
import numpy as np
import queue
import threading
import time

# Whisper ëª¨ë¸ ë¡œë“œ
print("ğŸ“¥ Whisper ëª¨ë¸ ë¡œë”© ì¤‘...")
whisper_model = whisper.load_model("base")  # small, medium, large ì„ íƒ ê°€ëŠ¥

# TTS ëª¨ë¸ ë¡œë“œ
print("ğŸ“¤ TTS ëª¨ë¸ ë¡œë”© ì¤‘...")
tts = TTS(model_name="tts_models/multilingual/multi-dataset/xtts_v2", progress_bar=True, gpu=False)

# ëŒ€ë‹µ ìƒì„±
def mock_chatgpt_response(user_text):
    # ì‹¤ì œ ChatGPT API ì—°ê²° ëŒ€ì‹  ê°„ë‹¨íˆ ì‘ë‹µ ì˜ˆì‹œ
    return f"ë‹¹ì‹ ì´ ì´ë ‡ê²Œ ë§í–ˆì–´ìš”: '{user_text}' ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”!"

# ë…¹ìŒ ì„¤ì •
samplerate = 16000
channels = 1
threshold = 500  # ìŒì„± ê°ì§€ ì„ê³„ê°’
silence_duration = 1.0  # ë¬´ìŒ ì‹œê°„ (ì´ˆ ë‹¨ìœ„)

q = queue.Queue()

# ì‹¤ì‹œê°„ ë…¹ìŒ ì½œë°±
def audio_callback(indata, frames, time_info, status):
    if status:
        print(status)
    q.put(indata.copy())

def record_dynamic(filename):
    print("ğŸ™ï¸ ëŒ€ê¸° ì¤‘... (ë§í•˜ë©´ ë…¹ìŒ ì‹œì‘)")
    recording = []
    silence_counter = 0
    speaking = False

    with sd.InputStream(samplerate=samplerate, channels=channels, callback=audio_callback, dtype='int16'):
        while True:
            try:
                data = q.get(timeout=1)
                volume_norm = np.linalg.norm(data) * 10

                if volume_norm > threshold:
                    if not speaking:
                        print("ğŸ¤ ìŒì„± ê°ì§€! ë…¹ìŒ ì‹œì‘")
                        speaking = True
                    recording.append(data)
                    silence_counter = 0
                else:
                    if speaking:
                        silence_counter += data.shape[0] / samplerate
                        recording.append(data)
                        if silence_counter > silence_duration:
                            print("ğŸ›‘ ìŒì„± ì¢…ë£Œ ê°ì§€")
                            break
            except queue.Empty:
                continue

    recording = np.concatenate(recording, axis=0)
    sf.write(filename, recording, samplerate)

# ìŒì„± ì¬ìƒ
def play_audio(file_path):
    data, samplerate = sf.read(file_path)
    sd.play(data, samplerate)
    sd.wait()

def main():
    input_audio = "user_input.wav"
    speaker_audio = "audio_test_ko.wav"

    while True:
        # 1. ì‚¬ëŒ ë§í•  ë•Œê¹Œì§€ ëŒ€ê¸° + ë…¹ìŒ
        record_dynamic(input_audio)

        # 2. Whisper ë³€í™˜
        print("ğŸ§  ìŒì„± â†’ í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘...")
        result = whisper_model.transcribe(input_audio)
        user_text = result["text"]
        print(f"ğŸ“ ì‚¬ìš©ì: {user_text}")

        # 3. ChatGPT ë‹µë³€
        response_text = mock_chatgpt_response(user_text)
        print(f"ğŸ¤– ë‹µë³€: {response_text}")

        # 4. TTS ë³€í™˜
        output_audio = "response.wav"
        tts.tts_to_file(
            text=response_text,
            file_path=output_audio,
            speaker_wav=speaker_audio,
            language="ko"
        )
        print(f"ğŸ”Š ì‘ë‹µ ìƒì„± ì™„ë£Œ: {output_audio}")

        # 5. ì¬ìƒ
        play_audio(output_audio)

if __name__ == "__main__":
    main()
